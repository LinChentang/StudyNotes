# Task1 引言

- 语言模型
- 历史回顾

## 语言模型

#### 1.什么是语言模型?

&emsp;&emsp;在自然语言处理领域，语言模型（ $Language Model$ ,`LM`）的经典定义是**一种对词符序列( $token$ )的概率分布**.即(对于任意的词序列，它能够计算出这个序列是一句话的概率)

&emsp;&emsp;语言模型p为每个令牌序列  $x_{1},...,x_{L}$ 分配一个概率（介于0和1之间的数字）：

$$
p(x_1, \dots, x_L)
$$

&emsp;&emsp;语言模型 ( $p$ ) 接收一串单词，然后计算出这个序列在自然语言中出现的概率，帮助我们判断这个句子是好是坏。但除了评估，我们还可以利用语言模型来产生文本。

&emsp;&emsp;从文本生成角度来说语言模型定义是给定一个短语(一个词组或一句话)，语言模型可以生成(预测)接下来的一个词.

#### 2.自回归语言模型

&emsp;&emsp;自回归语言模型（ $Autoregressive Language Models$, `ALMs`）在文本生成起到核心作用.

&emsp;&emsp;**原理**:模型试图理解，给定之前的单词，下一个单词是什么的可能性.(模型通过一个称为**概率的链式法则**的数学公式来工作)

&emsp;&emsp;将文本序列抽象为  $x_{1:L}$ 的联合分布 $p(x_{1:L})$ 表示这个自回归语言模型预测下一个词的概率，使用概率的链式法常见写法是：

$$
p(x_{1:L}) = p(x_1) p(x_2 \mid x_1) p(x_3 \mid x_1, x_2) \cdots p(x_L \mid x_{1:L-1}) = \prod_{i=1}^L p(x_i \mid x_{1:i-1}).
$$

&emsp;&emsp;在自回归语言模型 $p$ 中生成整个序列 $x_{1:L}$ 我们需要一次生成一个词元( $token$ )，该词元基于之前生成的词元( $token$ )进行计算获得：

$$\begin{aligned}
\text { for } i & =1, \ldots, L: \\
x_i & \sim p\left(x_i \mid x_{1: i-1}\right)^{1 / T}
\end{aligned}$$

其中 $T≥0$ 是一个控制我们希望从语言模型中得到多少随机性的温度参数：
- T=0：确定性地在每个位置 $i$ 选择最可能的词元 $x_{i}$
- T=1：从纯语言模型“正常（ $normally$ ）”采样
- T=∞：从整个词汇表上的均匀分布中采样
然而，如果我们仅将概率提高到  $1/T$  的次方，概率分布可能不会加和到 $1$ 。我们可以通过重新标准化分布来解决这个问题。我们将标准化版本  $p_{T}(x_{i}Ox_{1:i?1})∝p(x_{i}Ox_{1:i?1})^{1/T}$ 称为退火条件概率分布。

对于非自回归的条件生成，我们可以通过指定某个前缀序列 $x_{1:i}$ （称为提示）并采样其余的  $x_{i+1:L}$ （称为补全）来进行条件生成。

## 历史回顾

#### 1.信息理论

&emsp;&emsp;1948年克劳德・香农在论文《通信的数学理论》中引入用于度量概率分布的熵（Entropy）的概念：
$$ H(p) = \sum_x p(x) \log \frac{1}{p(x)}$$
熵实际上是一个衡量将样本$x?p$ 编码（即压缩）成比特串所需要的预期比特数的度量。

熵的值越小，表明序列的结构性越强，编码的长度就越短。直观地理解， $\log \frac{1}{p(x)}$  可以视为用于表示出现概率为 $p(x)$ 的元素 $x$ 的编码的长度。

比如：如果 $p(x)=1/8$ ，我们就需要分配  $log_{2}(8)=3$ 个比特（或等价地， $log(8)=2.08$ 个自然单位）。

需要注意的是，实际上达到香农极限（Shannon limit）是非常具有挑战性的（例如，低密度奇偶校验码），这也是编码理论研究的主题之一

#### 2.英语的熵

&emsp;&emsp;英语的熵，即英语文本中信息的平均不确定性或随机性，可以通过计算交叉熵的平均值得到。
交叉熵的定义：
$$H(p, q)=-\sum_x p(x) \log q(x)$$

#### 3.n-gram模型

&emsp;&emsp;语言模型首先被用于需要生成文本的实践应用：

1970年代的语音识别（输入：声音信号，输出：文本）
1990年代的机器翻译（输入：源语言的文本，输出：目标语言的文本）

$p(\text{text} \mid \text{speech}) \propto \underbrace{p(\text{text})}\text{language model} \underbrace{p(\text{speech} \mid \text{text})}\text{acoustic model}.$

语音识别和机器翻译系统使用了基于词的n-gram语言模型（最早由香农引入，但针对的是字符）。

N-gram模型。在一个n-gram模型中，关于$x_{i}$的预测只依赖于最后的 $n-1$ 个字符 $x_{i?(n?1):i?1}$ ，而不是整个历史：

$$
p(x_i \mid x_{1:i-1}) = p(x_i \mid x_{i-(n-1):i-1}).
$$

#### 4.神经语言模型

&emsp;&emsp;其中 $p(x_{i}Ox_{i?(n?1):i?1})$ 由神经网络给出：

$$
p(cheeseOate,the)=some-neural-network(ate,the,cheese)。
$$
上下文长度仍然受到n的限制，但现在对更大的n值估计神经语言模型在统计上是可行的。

然而，主要的挑战是训练神经网络在计算上要昂贵得多。他们仅在1400万个词上训练了一个模型，并显示出它在相同数据量上优于n-gram模型。但由于n-gram模型的扩展性更好，且数据并非瓶颈，所以n-gram模型在至少接下来的十年中仍然占主导地位。

&emsp;&emsp;神经语言建模的两个关键发展

- **$Recurrent Neural Networks$**(`RNNs`),包括长短期记忆(`LSTMs`),使得一个词元 $x_{i}$ 的条件分布可以依赖于整个上下文 $x_{1:i?1}$（有效地使 $n=∞$）,但这些模型难以训练.
- **$Transformers$**是一个较新的架构(于2017年为机器翻译开发),再次返回固定上下文长度 $n$ ,但更易于训练(并利用了`GPU`的并行性).此外,$n$ 可以对许多应用程序“足够大”( $GPT-3$ 使用的是 $n=2048$ ).